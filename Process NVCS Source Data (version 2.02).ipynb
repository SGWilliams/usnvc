{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook lays out and runs the process for USNVC source data into a MongoDB store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os,requests,uuid\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from bis2 import dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, I dumped the files here into the working code repo to deal with them locally. Eventually, we will want to put these into their own source code repo or somehow deal with the version control dynamic on the files. I output the file list here to see what all I'm dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['unitObsoleteParent.txt',\n",
       " 'reference.txt',\n",
       " 'unit.txt',\n",
       " 'UnitXReference.txt',\n",
       " 'unitObsoleteName.txt',\n",
       " 'UnitXEcoregionUsfs2007.txt',\n",
       " 'UnitXSubnation.txt',\n",
       " 'unitPredecessor.txt',\n",
       " 'd_usfs_ecoregion1994.txt',\n",
       " 'unitDescription.txt',\n",
       " 'd_dist_confidence.txt',\n",
       " 'd_occurrence_status.txt',\n",
       " 'UnitXSimilarUnit.txt',\n",
       " 'd_curr_presence_absence.txt',\n",
       " 'UnitXEcoregionUsfs1994.txt',\n",
       " 'd_usfs_ecoregion_level.txt',\n",
       " 'd_usfs_ecoregion2007.txt']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['d_spatial_pattern.txt',\n",
       " 'd_classif_confidence.txt',\n",
       " 'd_classification_level.txt',\n",
       " 'd_subnation.txt']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "usnvcExportFolder = \"USNVC v2.02 export 2018-03\"\n",
    "display (os.listdir(usnvcExportFolder))\n",
    "usnvcDefinitionTablesFolder = \"usnvc_definition_tables\"\n",
    "display (os.listdir(usnvcDefinitionTablesFolder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Attributes, Hierarchy, and Descriptions\n",
    "The following code block merges the unit and unit description tables into one dataframe that serves as the core data for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element_global_id             object\n",
      "parent_id                     object\n",
      "d_classification_level_id      int64\n",
      "elementuid                   float64\n",
      "classificationcode            object\n",
      "databasecode                  object\n",
      "status                        object\n",
      "colloquialname                object\n",
      "scientificname                object\n",
      "formattedscientificname       object\n",
      "translatedname                object\n",
      "hierarchylevel                object\n",
      "unitsort                      object\n",
      "usstatus                      object\n",
      "typeconceptsentence           object\n",
      "parentkey                     object\n",
      "parentname                    object\n",
      "typeconcept                   object\n",
      "diagnosticcharacteristics     object\n",
      "rationale                     object\n",
      "classificationcomments        object\n",
      "similarnvctypescomments       object\n",
      "physiognomy                   object\n",
      "floristics                    object\n",
      "plotcount                    float64\n",
      "dynamics                      object\n",
      "environment                   object\n",
      "range                         object\n",
      "nations                       object\n",
      "subnations                    object\n",
      "tncecoregions                 object\n",
      "usfsecoregions1994            object\n",
      "usfsecoregions2007            object\n",
      "omernikecoregions             object\n",
      "federallands                  object\n",
      "spatialpattern                object\n",
      "plotsummary                   object\n",
      "plottypal                     object\n",
      "plotarchived                  object\n",
      "plotconsistency               object\n",
      "plotsize                      object\n",
      "plotmethods                   object\n",
      "classif_confidence_id         object\n",
      "confidencecomments            object\n",
      "grank                         object\n",
      "grankreviewdate               object\n",
      "grankauthor                   object\n",
      "grankreasons                  object\n",
      "othercomments                 object\n",
      "lineage                       object\n",
      "synonymy                      object\n",
      "primaryconceptsource          object\n",
      "descriptionauthor             object\n",
      "acknowledgements              object\n",
      "versiondate                   object\n",
      "D_CLASSIF_CONFIDENCE_ID        int64\n",
      "CLASSIF_CONFIDENCE_DESC       object\n",
      "DISPLAY_ORDER                  int64\n",
      "STATUS                        object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "units = pd.read_csv(usnvcExportFolder+\"/unit.txt\", sep='\\t', encoding = \"ISO-8859-1\", dtype={\"element_global_id\":str,\"parent_id\":str,\"classif_confidence_id\":int})\n",
    "unitDescriptions = pd.read_csv(usnvcExportFolder+\"/unitDescription.txt\", sep='\\t', encoding = \"ISO-8859-1\", dtype={\"element_global_id\":str})\n",
    "codes_classificationConfidence = pd.read_csv(usnvcDefinitionTablesFolder+\"/d_classif_confidence.txt\", sep='\\t', encoding = \"ISO-8859-1\")\n",
    "\n",
    "nvcsUnits = pd.merge(left=units,right=unitDescriptions, left_on='element_global_id', right_on='element_global_id')\n",
    "nvcsUnits = pd.merge(left=nvcsUnits,right=codes_classificationConfidence, left_on='classif_confidence_id', right_on='D_CLASSIF_CONFIDENCE_ID')\n",
    "print (nvcsUnits.dtypes)\n",
    "\n",
    "del units\n",
    "del unitDescriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit References\n",
    "The following dataframes assemble the unit by unit references into a merged dataframe for later query and processing when building the unit documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element_global_id    object\n",
      "reference_id         object\n",
      "shortcitation        object\n",
      "fullcitation         object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "unitByReference = pd.read_csv(usnvcExportFolder+\"/UnitXReference.txt\", sep='\\t', encoding = \"ISO-8859-1\", dtype={\"element_global_id\":str,\"reference_id\":str})\n",
    "references = pd.read_csv(usnvcExportFolder+\"/reference.txt\", sep='\\t', encoding = \"ISO-8859-1\", dtype={\"reference_id\":str})\n",
    "unitReferences = pd.merge(left=unitByReference,right=references, left_on='reference_id', right_on='reference_id')\n",
    "\n",
    "print (unitReferences.dtypes)\n",
    "\n",
    "del unitByReference\n",
    "del references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Predecessors\n",
    "The following codeblock retrieves the unit predecessors for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element_global_id            object\n",
      "predecessor_id               object\n",
      "predecessorcode              object\n",
      "predecessorname              object\n",
      "predecessorsciname           object\n",
      "predecessorcolloquialname    object\n",
      "lineagedate                  object\n",
      "lineagenote                  object\n",
      "lineageauthorizedby          object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "unitPredecessors = pd.read_csv(usnvcExportFolder+\"/UnitPredecessor.txt\", sep='\\t', encoding = \"ISO-8859-1\", dtype={\"element_global_id\":str,\"predecessor_id\":str})\n",
    "\n",
    "print(unitPredecessors.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obsolete records\n",
    "The following codeblock retrieves the two tables that contain references to obsolete units or names. We may want to examine this in future versions to move from simply capturing notes about obsolescence to keeping track of what is actually changing. Alternatively, we can keep with a whole dataset versioning construct if that works better for the community, but as soon as we start minting individual DOIs for the units, making them citable, that changes the dynamic in how we manage the data moving forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element_global_id    object\n",
      "obsoletename         object\n",
      "obsoletenote         object\n",
      "obsoletedate         object\n",
      "obsoleteauthority    object\n",
      "dtype: object\n",
      "element_global_id     object\n",
      "obsoleteparentcode    object\n",
      "obsoletedivision      object\n",
      "obsoleteparentname    object\n",
      "obsoletenote          object\n",
      "obsoletedate          object\n",
      "obsoleteauthority     object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "obsoleteUnits = pd.read_csv(usnvcExportFolder+\"/UnitObsoleteName.txt\", sep='\\t', encoding = \"ISO-8859-1\", dtype={\"element_global_id\":str})\n",
    "print (obsoleteUnits.dtypes)\n",
    "\n",
    "obsoleteParents = pd.read_csv(usnvcExportFolder+\"/UnitObsoleteParent.txt\", sep='\\t', encoding = \"ISO-8859-1\", dtype={\"element_global_id\":str})\n",
    "print (obsoleteParents.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Distribution - Nations and Subnations\n",
    "The following codeblock assembles the four tables that make up all the code references for the unit by unit distribution at the national level and then in North American states and provinces. I played around with adding a little bit of value to the nations structure by looking up names and setting up objects that contain name, abbreviation, uncertainty (true/false), and an info API reference. But I also kept the original raw string/list of national abbreviations. That process would be a lot smarter if I did it here by pulling together a distinct list of all referenced nation codes/abbreviations and then building a lookup dataframe on those. I'll revisit at some point or if the code bogs down, but the REST API call is pretty quick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element_global_id             object\n",
      "subnation_id                  object\n",
      "d_curr_presence_absence_id    object\n",
      "d_dist_confidence_id          object\n",
      "D_CURR_PRESENCE_ABSENCE_ID    object\n",
      "CURR_PRESENCE_ABSENCE_DESC    object\n",
      "CURR_PRESENCE_ABSENCE_CD      object\n",
      "D_DIST_CONFIDENCE_ID          object\n",
      "DIST_CONFIDENCE_CD            object\n",
      "DIST_CONFIDENCE_DESC          object\n",
      "iso_nation_cd                 object\n",
      "subnation_code                object\n",
      "subnation_name                object\n",
      "dtype: object 427336\n"
     ]
    }
   ],
   "source": [
    "unitXSubnation = pd.read_csv(usnvcExportFolder+\"/UnitXSubnation.txt\", sep='\\t', encoding = \"ISO-8859-1\", dtype=str)\n",
    "codes_CurrentPresAbs = pd.read_csv(usnvcExportFolder+\"/d_curr_presence_absence.txt\", sep='\\t', encoding = \"ISO-8859-1\", dtype=str)\n",
    "codes_DistConfidence = pd.read_csv(usnvcExportFolder+\"/d_dist_confidence.txt\", sep='\\t', encoding = \"ISO-8859-1\", dtype=str)\n",
    "codes_Subnations = pd.read_csv(usnvcDefinitionTablesFolder+\"/d_subnation.txt\", sep='\\t', encoding = \"ISO-8859-1\", dtype=str)\n",
    "\n",
    "nvcsDistribution = pd.merge(left=unitXSubnation,right=codes_CurrentPresAbs, left_on='d_curr_presence_absence_id', right_on='D_CURR_PRESENCE_ABSENCE_ID')\n",
    "nvcsDistribution = pd.merge(left=nvcsDistribution,right=codes_DistConfidence, left_on='d_dist_confidence_id', right_on='D_DIST_CONFIDENCE_ID')\n",
    "nvcsDistribution = pd.merge(left=nvcsDistribution,right=codes_Subnations, left_on='subnation_id', right_on='subnation_id')\n",
    "\n",
    "print (nvcsDistribution.dtypes, nvcsDistribution.size)\n",
    "\n",
    "del unitXSubnation\n",
    "del codes_CurrentPresAbs\n",
    "del codes_DistConfidence\n",
    "del codes_Subnations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USFS Ecoregions\n",
    "There is a coded list of USFS Ecoregion information in the unit descriptions, but this would have to be parsed and referenced out anyway and the base information seems to come through a \"unitX...\" set of tables. This codeblock sets those data up for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "element_global_id            object\n",
      "usfs_ecoregion_id            object\n",
      "d_occurrence_status_id       object\n",
      "USFS_ECOREGION_ID            object\n",
      "PARENT_USFS_ECOREGION_ID     object\n",
      "D_USFS_ECOREGION_LEVEL_ID    object\n",
      "USFS_ECOREGION_NAME          object\n",
      "USFS_ECOREGION_CLASS_CD      object\n",
      "USFS_ECOREGION_CONCAT_CD     object\n",
      "D_OCCURRENCE_STATUS_ID       object\n",
      "OCCURRENCE_STATUS_CD         object\n",
      "OCCURRENCE_STATUS_DESC       object\n",
      "dtype: object\n",
      "----------\n",
      "element_global_id                object\n",
      "usfs_ecoregion_2007_id           object\n",
      "d_occurrence_status_id           object\n",
      "parent_usfs_ecoregion_2007_id    object\n",
      "d_usfs_ecoregion_level_id        object\n",
      "usfs_ecoregion_2007_name         object\n",
      "usfs_ecoregion_2007_concat_cd    object\n",
      "D_OCCURRENCE_STATUS_ID           object\n",
      "OCCURRENCE_STATUS_CD             object\n",
      "OCCURRENCE_STATUS_DESC           object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "unitXUSFSEcoregion1994 = pd.read_csv(usnvcExportFolder+\"/UnitXEcoregionUsfs1994.txt\", sep='\\t', encoding = \"ISO-8859-1\", dtype=str)\n",
    "codes_USFSEcoregions1994 = pd.read_csv(usnvcExportFolder+\"/d_usfs_ecoregion1994.txt\", sep='\\t', encoding = \"ISO-8859-1\", dtype=str)\n",
    "\n",
    "unitXUSFSEcoregion2007 = pd.read_csv(usnvcExportFolder+\"/UnitXEcoregionUsfs2007.txt\", sep='\\t', encoding = \"ISO-8859-1\", dtype=str)\n",
    "codes_USFSEcoregions2007 = pd.read_csv(usnvcExportFolder+\"/d_usfs_ecoregion2007.txt\", sep='\\t', encoding = \"ISO-8859-1\", dtype=str)\n",
    "\n",
    "codes_OccurrenceStatus = pd.read_csv(usnvcExportFolder+\"/d_occurrence_status.txt\", sep='\\t', encoding = \"ISO-8859-1\", dtype=str)\n",
    "\n",
    "usfsEcoregionDistribution1994 = pd.merge(left=unitXUSFSEcoregion1994,right=codes_USFSEcoregions1994, left_on='usfs_ecoregion_id', right_on='USFS_ECOREGION_ID')\n",
    "usfsEcoregionDistribution1994 = pd.merge(left=usfsEcoregionDistribution1994,right=codes_OccurrenceStatus, left_on='d_occurrence_status_id', right_on='D_OCCURRENCE_STATUS_ID')\n",
    "\n",
    "usfsEcoregionDistribution2007 = pd.merge(left=unitXUSFSEcoregion2007,right=codes_USFSEcoregions2007, left_on='usfs_ecoregion_2007_id', right_on='usfs_ecoregion_2007_id')\n",
    "usfsEcoregionDistribution2007 = pd.merge(left=usfsEcoregionDistribution2007,right=codes_OccurrenceStatus, left_on='d_occurrence_status_id', right_on='D_OCCURRENCE_STATUS_ID')\n",
    "\n",
    "print (usfsEcoregionDistribution1994.dtypes)\n",
    "print (\"----------\")\n",
    "print (usfsEcoregionDistribution2007.dtypes)\n",
    "\n",
    "del unitXUSFSEcoregion1994\n",
    "del codes_USFSEcoregions1994\n",
    "del unitXUSFSEcoregion2007\n",
    "del codes_USFSEcoregions2007\n",
    "del codes_OccurrenceStatus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm concluding (for the moment) that we should use element_global_id as a unique and persistent opaque identifier for a given unit in this data system over time. We still need to decide if we will adopt this as the core identifier for our purposes or assign our own for absolute certainty since we don't really know what all rules apply in the NatureServe Biotics system but suspect some level of squishiness given the multiple identifiers that seem to be in their data now. The \"elementuid\" is just a \"2.\" with the element_global_id - don't know what that means, so maybe we can ignore it unless someone tells us something useful.\n",
    "\n",
    "Both databasecode and classificationcode (don't understand the significance of the attribute names) have some part to play in establishing display titles for the units in the application. Certain databasecode values also appear in the one posted proceedings document from the ESA panel, where they seem to be used as meaningful shorthand (kind of a PITA, but as long as its consistent...). It appears that classificationcode is prefixed on Class, Subclass, Formation, and Division names and databasecode is prefixed on Macrogroup, Group, Alliance, and Association units to produce working full titles.\n",
    "\n",
    "A different set of rules seems to apply for the Cultural units where the current app seems to be conflicting in terms of what is shown at a unit document level and what shows up in search results. Argh! We'll see if we can get a consistent ruleset nailed down that relates the identifiers to title and shorthand conventions so we better know how to package and present.\n",
    "\n",
    "parent_id appears to be a reference to a parent element_global_id, establishing the hierarchy, though we need to be careful about number handling with Pandas so I explicitly made both those IDs strings on the way in.\n",
    "\n",
    "Given what we've discovered so far, we probably have enough information to start laying out the unit data structure we want. Assuming we might make one overall pass at establishing initial records, at least, I build this into a list of documents. A few conventions I'm playing with here:\n",
    "\n",
    "* Assign more human-friendly attribute names to the things that we will display to people, but retain a few of the \"ugly names\" for things that have special meaning in the data assembly process\n",
    "* Build logical buckets of information into sub-documents, but don't go too crazy in deeply nesting the structure (at least for now as we're working with better understanding the data)\n",
    "\n",
    "There appears to be some messiness in the values, so the cleanString function can be tweaked as we work through to fix problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanString(text):\n",
    "    replacements = {'&amp;': '&','&lt;':'<','&gt;':'>'}\n",
    "    for x,y in replacements.items():\n",
    "        text = text.replace(x, y)\n",
    "    return (text)\n",
    "\n",
    "def getHierarchyFromDF(element_global_id):\n",
    "    # Assumes the full dataframe exists in memory here already\n",
    "    thisUnitData = nvcsUnits.loc[nvcsUnits[\"element_global_id\"] == str(element_global_id), [\"element_global_id\",\"parent_id\",\"hierarchylevel\",\"classificationcode\",\"databasecode\",\"translatedname\",\"colloquialname\",\"unitsort\",\"DISPLAY_ORDER\"]]\n",
    "    \n",
    "    immediateChildren = nvcsUnits.loc[nvcsUnits[\"parent_id\"] == str(element_global_id), [\"element_global_id\",\"parent_id\",\"hierarchylevel\",\"classificationcode\",\"databasecode\",\"translatedname\",\"colloquialname\",\"unitsort\",\"DISPLAY_ORDER\"]]\n",
    "\n",
    "    parentID = thisUnitData[\"parent_id\"].values[0]\n",
    "\n",
    "    ancestors = []\n",
    "    while type(parentID) is str:\n",
    "        ancestor = nvcsUnits.loc[nvcsUnits[\"element_global_id\"] == str(parentID), [\"element_global_id\",\"parent_id\",\"hierarchylevel\",\"classificationcode\",\"databasecode\",\"translatedname\",\"colloquialname\",\"unitsort\",\"DISPLAY_ORDER\"]]\n",
    "        ancestors = ancestors + ancestor.to_dict(\"records\")\n",
    "        parentID = ancestor[\"parent_id\"].values[0]\n",
    "        \n",
    "    hierarchyList = []\n",
    "    for record in ancestors+thisUnitData.to_dict(\"records\")+immediateChildren.to_dict(\"records\"):\n",
    "        if record[\"hierarchylevel\"] in [\"Class\",\"Subclass\",\"Formation\",\"Division\"]:\n",
    "            record[\"Display Title\"] = record[\"classificationcode\"]+\" \"+record[\"colloquialname\"]+\" \"+record[\"hierarchylevel\"]\n",
    "        elif record[\"hierarchylevel\"] in [\"Macrogroup\",\"Group\"]:\n",
    "            record[\"Display Title\"] = record[\"classificationcode\"]+\" \"+record[\"translatedname\"]\n",
    "        else:\n",
    "            record[\"Display Title\"] = record[\"databasecode\"]+\" \"+record[\"translatedname\"]\n",
    "        hierarchyList.append(record)\n",
    "    \n",
    "    return hierarchyList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvcsUnitDocs = []\n",
    "for index,row in nvcsUnits.iterrows():\n",
    "    #if index > 19:\n",
    "    #    break\n",
    "    unitDoc = {\"Identifiers\":{},\"Overview\":{},\"Hierarchy\":{},\"Vegetation\":{},\"Environment\":{},\"Distribution\":{},\"Plot Sampling and Analysis\":{},\"Confidence Level\":{},\"Conservation Status\":{},\"Hierarchy\":{},\"Concept History\":{},\"Synonymy\":{},\"Authorship\":{},\"References\":[]}\n",
    "\n",
    "    unitDoc[\"_id\"] = str(uuid.uuid4())\n",
    "\n",
    "    unitDoc[\"Identifiers\"][\"element_global_id\"] = row[\"element_global_id\"]\n",
    "    unitDoc[\"Identifiers\"][\"Database Code\"] = row[\"databasecode\"]\n",
    "    unitDoc[\"Identifiers\"][\"Classification Code\"] = row[\"classificationcode\"]\n",
    "\n",
    "    unitDoc[\"Overview\"][\"Scientific Name\"] = row[\"scientificname\"]\n",
    "    unitDoc[\"Overview\"][\"Formatted Scientific Name\"] = cleanString(row[\"formattedscientificname\"])\n",
    "    unitDoc[\"Overview\"][\"Translated Name\"] = row[\"translatedname\"]\n",
    "    if type(row[\"colloquialname\"]) is str:\n",
    "        unitDoc[\"Overview\"][\"Colloquial Name\"] = row[\"colloquialname\"]\n",
    "    if type(row[\"typeconceptsentence\"]) is str:\n",
    "        unitDoc[\"Overview\"][\"Type Concept Sentence\"] = cleanString(row[\"typeconceptsentence\"])\n",
    "    if type(row[\"typeconcept\"]) is str:\n",
    "        unitDoc[\"Overview\"][\"Type Concept\"] = cleanString(row[\"typeconcept\"])\n",
    "    if type(row[\"diagnosticcharacteristics\"]) is str:\n",
    "        unitDoc[\"Overview\"][\"Diagnostic Characteristics\"] = cleanString(row[\"diagnosticcharacteristics\"])\n",
    "    if type(row[\"rationale\"]) is str:\n",
    "        unitDoc[\"Overview\"][\"Rationale for Nonimal Species or Physiognomic Features\"] = cleanString(row[\"rationale\"])\n",
    "    if type(row[\"classificationcomments\"]) is str:\n",
    "        unitDoc[\"Overview\"][\"Classification Comments\"] = cleanString(row[\"classificationcomments\"])\n",
    "    if type(row[\"similarnvctypescomments\"]) is str:\n",
    "        unitDoc[\"Overview\"][\"Similar NVC Types\"] = cleanString(row[\"similarnvctypescomments\"])\n",
    "    if type(row[\"othercomments\"]) is str:\n",
    "        unitDoc[\"Overview\"][\"Other Comments\"] = cleanString(row[\"othercomments\"])\n",
    "\n",
    "    if row[\"hierarchylevel\"] in [\"Class\",\"Subclass\",\"Formation\",\"Division\"]:\n",
    "        unitDoc[\"Overview\"][\"Display Title\"] = row[\"classificationcode\"]+\" \"+row[\"colloquialname\"]+\" \"+row[\"hierarchylevel\"]\n",
    "    elif row[\"hierarchylevel\"] in [\"Macrogroup\",\"Group\"]:\n",
    "        unitDoc[\"Overview\"][\"Display Title\"] = row[\"classificationcode\"]+\" \"+row[\"translatedname\"]\n",
    "    else:\n",
    "        unitDoc[\"Overview\"][\"Display Title\"] = row[\"databasecode\"]+\" \"+row[\"translatedname\"]\n",
    "    \n",
    "    if type(row[\"physiognomy\"]) is str:\n",
    "        unitDoc[\"Vegetation\"][\"Physiognomy and Structure\"] = cleanString(row[\"physiognomy\"])\n",
    "    if type(row[\"floristics\"]) is str:\n",
    "        unitDoc[\"Vegetation\"][\"Floristics\"] = cleanString(row[\"floristics\"])\n",
    "    if type(row[\"dynamics\"]) is str:\n",
    "        unitDoc[\"Vegetation\"][\"Dynamics\"] = cleanString(row[\"dynamics\"])\n",
    "    \n",
    "    if type(row[\"environment\"]) is str:\n",
    "        unitDoc[\"Environment\"][\"Environmental Description\"] = cleanString(row[\"environment\"])\n",
    "\n",
    "    if type(row[\"spatialpattern\"]) is str:\n",
    "        unitDoc[\"Environment\"][\"Spatial Pattern\"] = cleanString(row[\"spatialpattern\"])\n",
    "\n",
    "    if type(row[\"range\"]) is str:\n",
    "        unitDoc[\"Distribution\"][\"Geographic Range\"] = row[\"range\"]\n",
    "\n",
    "    if type(row[\"nations\"]) is str:\n",
    "        unitDoc[\"Distribution\"][\"Nations\"] = {\"Raw List\":row[\"nations\"],\"Nation Info\":[]}\n",
    "        for nation in unitDoc[\"Distribution\"][\"Nations\"][\"Raw List\"].split(\",\"):\n",
    "            thisNation = {\"Abbreviation\":nation.replace(\"?\",\"\").strip()}\n",
    "            if nation.endswith(\"?\"):\n",
    "                thisNation[\"Uncertainty\"] = True\n",
    "            else:\n",
    "                thisNation[\"Uncertainty\"] = False\n",
    "            thisNation[\"Info API\"] = \"https://restcountries.eu/rest/v2/alpha/\"+thisNation[\"Abbreviation\"]\n",
    "            thisNationInfo = requests.get(thisNation[\"Info API\"]+\"?fields=name\").json()\n",
    "            if \"name\" in thisNationInfo.keys():\n",
    "                thisNation[\"Name\"] = thisNationInfo[\"name\"]\n",
    "            unitDoc[\"Distribution\"][\"Nations\"][\"Nation Info\"].append(thisNation)\n",
    "    \n",
    "    if type(row[\"subnations\"]) is str:\n",
    "        unitDoc[\"Distribution\"][\"Subnations\"] = {\"Raw List\":row[\"subnations\"]}\n",
    "\n",
    "    thisDistribution = nvcsDistribution.loc[nvcsDistribution[\"element_global_id\"] == row[\"element_global_id\"]]\n",
    "    if len(thisDistribution.index) > 0:\n",
    "        unitDoc[\"Distribution\"][\"States/Provinces Raw Data\"] = thisDistribution.to_dict(\"records\")\n",
    "    \n",
    "    thisUSFSDistribution1994 = usfsEcoregionDistribution1994.loc[usfsEcoregionDistribution1994[\"element_global_id\"] == row[\"element_global_id\"]]\n",
    "    if len(thisUSFSDistribution1994.index) > 0:\n",
    "        unitDoc[\"Distribution\"][\"1994 USFS Ecoregion Raw Data\"] = thisUSFSDistribution1994.to_dict(\"records\")\n",
    "    \n",
    "    thisUSFSDistribution2007 = usfsEcoregionDistribution2007.loc[usfsEcoregionDistribution2007[\"element_global_id\"] == row[\"element_global_id\"]]\n",
    "    if len(thisUSFSDistribution2007.index) > 0:\n",
    "        unitDoc[\"Distribution\"][\"2007 USFS Ecoregion Raw Data\"] = thisUSFSDistribution2007.to_dict(\"records\")\n",
    "\n",
    "    if type(row[\"tncecoregions\"]) is int:\n",
    "        unitDoc[\"Distribution\"][\"TNC Ecoregions\"] = row[\"tncecoregions\"]\n",
    "\n",
    "    if type(row[\"omernikecoregions\"]) is int:\n",
    "        unitDoc[\"Distribution\"][\"Omernik Ecoregions\"] = row[\"omernikecoregions\"]\n",
    "\n",
    "    if type(row[\"omernikecoregions\"]) is int:\n",
    "        unitDoc[\"Distribution\"][\"Omernik Ecoregions\"] = row[\"omernikecoregions\"]\n",
    "\n",
    "    if type(row[\"federallands\"]) is int:\n",
    "        unitDoc[\"Distribution\"][\"Federal Lands\"] = row[\"federallands\"]\n",
    "\n",
    "    if type(row[\"plotcount\"]) is int:\n",
    "        unitDoc[\"Plot Sampling and Analysis\"][\"Plot Count\"] = row[\"plotcount\"]\n",
    "    if type(row[\"plotsummary\"]) is str:\n",
    "        unitDoc[\"Plot Sampling and Analysis\"][\"Plot Summary\"] = row[\"plotsummary\"]\n",
    "    if type(row[\"plottypal\"]) is str:\n",
    "        unitDoc[\"Plot Sampling and Analysis\"][\"Plot Type\"] = row[\"plottypal\"]\n",
    "    if type(row[\"plotarchived\"]) is str:\n",
    "        unitDoc[\"Plot Sampling and Analysis\"][\"Plot Archive\"] = row[\"plotarchived\"]\n",
    "    if type(row[\"plotconsistency\"]) is str:\n",
    "        unitDoc[\"Plot Sampling and Analysis\"][\"Plot Consistency\"] = row[\"plotconsistency\"]\n",
    "    if type(row[\"plotsize\"]) is str:\n",
    "        unitDoc[\"Plot Sampling and Analysis\"][\"Plot Size\"] = row[\"plotsize\"]\n",
    "    if type(row[\"plotmethods\"]) is str:\n",
    "        unitDoc[\"Plot Sampling and Analysis\"][\"Plot Methods\"] = row[\"plotmethods\"]\n",
    "\n",
    "    unitDoc[\"Confidence Level\"][\"Confidence Level\"] = row[\"CLASSIF_CONFIDENCE_DESC\"]\n",
    "    if type(row[\"confidencecomments\"]) is str:\n",
    "        unitDoc[\"Confidence Level\"][\"Confidence Level Comments\"] = cleanString(row[\"confidencecomments\"])\n",
    "\n",
    "    if type(row[\"grank\"]) is str:\n",
    "        unitDoc[\"Conservation Status\"][\"Global Rank\"] = row[\"grank\"]\n",
    "    if type(row[\"grankreviewdate\"]) is str:\n",
    "        unitDoc[\"Conservation Status\"][\"Global Rank Review Date\"] = row[\"grankreviewdate\"]\n",
    "    if type(row[\"grankauthor\"]) is str:\n",
    "        unitDoc[\"Conservation Status\"][\"Global Rank Author\"] = row[\"grankauthor\"]\n",
    "    if type(row[\"grankreasons\"]) is str:\n",
    "        unitDoc[\"Conservation Status\"][\"Global Rank Reasons\"] = row[\"grankreasons\"]\n",
    "        \n",
    "    unitDoc[\"Hierarchy\"][\"parent_id\"] = str(row[\"parent_id\"])\n",
    "    unitDoc[\"Hierarchy\"][\"hierarchylevel\"] = row[\"hierarchylevel\"]\n",
    "    unitDoc[\"Hierarchy\"][\"d_classification_level_id\"] = row[\"d_classification_level_id\"]\n",
    "    unitDoc[\"Hierarchy\"][\"unitsort\"] = row[\"unitsort\"]\n",
    "    unitDoc[\"Hierarchy\"][\"parentkey\"] = row[\"parentkey\"]\n",
    "    unitDoc[\"Hierarchy\"][\"parentname\"] = row[\"parentname\"]\n",
    "    unitDoc[\"Hierarchy\"][\"Cached Hierarchy\"] = getHierarchyFromDF(row[\"element_global_id\"])\n",
    "\n",
    "    \n",
    "    if type(row[\"lineage\"]) is str:\n",
    "        unitDoc[\"Concept History\"][\"Concept Lineage\"] = row[\"lineage\"]\n",
    "    \n",
    "    thisUnitPredecessors = unitPredecessors.loc[unitPredecessors[\"element_global_id\"] == row[\"element_global_id\"]]\n",
    "    if len(thisUnitPredecessors.index) > 0:\n",
    "        unitDoc[\"Concept History\"][\"Predecessors Raw Data\"] = thisUnitPredecessors.to_dict(\"records\")\n",
    "\n",
    "    thisUnitObsoleteUnits = obsoleteUnits.loc[obsoleteUnits[\"element_global_id\"] == row[\"element_global_id\"]]\n",
    "    if len(thisUnitObsoleteUnits.index) > 0:\n",
    "        unitDoc[\"Concept History\"][\"Obsolete Units Raw Data\"] = thisUnitObsoleteUnits.to_dict(\"records\")\n",
    "\n",
    "    thisUnitObsoleteParents = obsoleteParents.loc[obsoleteParents[\"element_global_id\"] == row[\"element_global_id\"]]\n",
    "    if len(thisUnitObsoleteParents.index) > 0:\n",
    "        unitDoc[\"Concept History\"][\"Obsolete Parents Raw Data\"] = thisUnitObsoleteParents.to_dict(\"records\")\n",
    "\n",
    "    if type(row[\"synonymy\"]) is str:\n",
    "        unitDoc[\"Synonymy\"][\"Synonymy\"] = row[\"synonymy\"]\n",
    "\n",
    "    if type(row[\"primaryconceptsource\"]) is str:\n",
    "        unitDoc[\"Authorship\"][\"Concept Author\"] = row[\"primaryconceptsource\"]\n",
    "    if type(row[\"descriptionauthor\"]) is str:\n",
    "        unitDoc[\"Authorship\"][\"Description Author\"] = row[\"descriptionauthor\"]\n",
    "    if type(row[\"acknowledgements\"]) is str:\n",
    "        unitDoc[\"Authorship\"][\"Acknowledgements\"] = row[\"acknowledgements\"]\n",
    "    if type(row[\"versiondate\"]) is str:\n",
    "        unitDoc[\"Authorship\"][\"Version Date\"] = row[\"versiondate\"]\n",
    "    \n",
    "    thisUnitReferences = unitReferences.loc[unitReferences[\"element_global_id\"] == row[\"element_global_id\"]]\n",
    "    for index,row in thisUnitReferences.iterrows():\n",
    "        unitDoc[\"References\"].append({\"Short Citation\":row[\"shortcitation\"],\"Full Citation\":row[\"fullcitation\"]})\n",
    "\n",
    "    nvcsUnitDocs.append(unitDoc)\n",
    "#display (nvcsUnitDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bis = dd.getDB(\"bis\")\n",
    "nvcsCollection = bis[\"NVCS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x112657cf0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nvcsCollection.delete_many({})\n",
    "nvcsCollection.insert_many(nvcsUnitDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
